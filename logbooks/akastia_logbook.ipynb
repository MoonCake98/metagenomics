{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metagenomics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PHA bioplastic is more used as it is a better alternative compared to the plastic we know. But the longterm influence or the impact of the PHA is unknown. Especially if those bioplastic cups are thrown away and ends up on the soil. What kind of affects would it have on the microbiome and are there any changes in the microbiome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Research question\n",
    "This research aims to answer the question, how does the PHA/PHB (?) powder and pellets impact the microbiome of the soil?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27-02-25\n",
    "\n",
    "We had an interview with our client about the project we will do. As we did not get much information about our project we asked the following question, to help us know more about the project and what they wanted to see with the results.\n",
    "\n",
    "- Can you tell us about the project?\n",
    "The project started as MKB (a company through KCBBE) wanted to know how the happy cups, made of PHA bioplastic, would decompose. So they wanted to see the evidence of the decomposition of the happy cups. So they studied the results of the breakdown of the microbiome. They had three test groups; soil with no addition (control), soil with PHA powder (larger surface area, faster decomposition) and soil with PHA pellets (lesser surface area, slower decomposition).\n",
    "There were little soil quality data and no significant changes in the soil quality. The changes in the soil were mostly done based on the feeling, by seeing if the plant was growing or not. All the samples were equally cared for so the only variable is the samples. The conclusion they got was that the plant growth was mostly negatively affected by PHA powder compared to the pellets. The pellets were less affecting the plant but still noticeable. This could most likely be linked to the accelerated decompose, as this changes the microbiome of the soil, and the disturbance of the plant growth. \n",
    "It is clear from the 16s data about the bacteria, told Jeroen. He told us, if we have 18s data available (needs to be confirmed by Martijn), he wants to know what kind of fungus etc is in the soil.\n",
    "\n",
    "\n",
    "- What was the research question for this project\n",
    "The research question they had was, does the PHA have an influence on plant growth? \n",
    "From their research they got the result of the impact on plant growth, they want us to look into the the soil microbiome. What kind of impact did the PHA bioplastic have on the microbiome that led to the plant growth?\n",
    "\n",
    "- Are there any specific degradation pathways you are searching for?\n",
    "Not really, as \n",
    "\n",
    "- Did you also look at other factors that could influence plant growth?\n",
    "They did look at other factors but concluded it did not have a big impact, so they excluded it. \n",
    "\n",
    "- Is there availability of the soil condition, like the pH values, moisture level of the soil, etc?\n",
    "There is data on the soil condition, but it doesn't have an impact as it stayed the same. \n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "The microbiome soil data was collected by the KCBBE intern Ariana. The MinION data consists of 16S and WGS. \n",
    "\n",
    "The experiment setup was as mentioned of three test groups and looked at the plant growth if the PHA had an impact on the plants growth. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 28-02-25\n",
    " Looking up literatures that could help with understanding the project better and looking at possible tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool reaserch\n",
    "Reasons from why you choose this tools sort of intro\n",
    "3-03-25\n",
    "As we had to search for which tools we could use for our tools we got a mail that we only had minION 16s data and whole genome data. The tools we thought we could use, were not applicable for minION data. So we had to search for tools, that we were able to use in our data. \n",
    "\n",
    "5-03-25\n",
    "While searching for the tools, I found PEAR and Porechop that could possible be used for our data. The reason I thought that PEAR could be useful, is that we could use it as an pair end read merger. And for Porechop, I saw that a lot of researcher using porechop as a trimmer, and using it as a trimmer. So thought it would be good to look into PEAR and Porechop.\n",
    "In the class we as a group decided to split to work and look into tools and also alternatives. \n",
    "The one that I choose the look into was Porechop\n",
    "\n",
    "### Porechop \n",
    "Porechop is a tool that finds and removes adapters from Oxford Nanopore reads, which means MinION reads. This tool supports demultiplexing of the reads that were barcoded. When demultiplexing barcoded rads, we can use the --discard_middle (this is always active when demultiplexing). This is useful if there is a read with a middle adapter, this means it is highly likely it is chimeric reads. Chimeric reads, are also known as split reads, which are raeds that overlaps a DNA double strand breakingpoint.\n",
    "\n",
    "8-03-25\n",
    "Porechop can be modified with how you want it to align, trim and demultiplex.\n",
    "cons:\n",
    " - The part of the adapter-seach is increasingly slow.\n",
    " - It does only adapter search on subset of reads, so this means there can be problems with non-randomly ordered read sets\n",
    " - As many adapter share common sequence, it is possible that porechop finds the wrong adapter (you can give a kit or adapter with the porechop so it won't be an issue)\n",
    " - adapter sequence could be not properly basecalled, so this could result in inconsistent sequence.\n",
    "\n",
    "Porechop have to be installed, so look at the github link and follow the steps on how to download it. The command line which you could use is also on the github to find.\n",
    "\n",
    "porechop ref: https://github.com/rrwick/Porechop?tab=readme-ov-file\n",
    "\n",
    "### Other tools \n",
    "\n",
    "As fastqc is not suitable for MinION reads, it is suggested to use another tool.\n",
    "The tool we could use is; \n",
    "minion_qc : This tool is fast and effective for our data. It can handle big data set, it is written in R. This tool gives a range of diagnostic plots and data for quality control. \n",
    "nano_qc: This tool can handle long read sequencing and replicate some of the same plots that fastQC makes. This can be installed via bioconda and with pip.\n",
    "toulligQC: This tool is made especially for making QC analyses minION data. It is relatively new and not used by a lot of people.\n",
    "fastplong: fast preprocesser and does quality control for long reads.\n",
    "fastq-scan : gives summary statistics of an input fastq in json.\n",
    "\n",
    "minion_qc: https://github.com/roblanf/minion_qc (multiqc has minion_qc modules)\n",
    "nano_qc: https://github.com/wdecoster/nanoQC\n",
    "toullingQC ling: https://github.com/GenomiqueENS/toulligQC\n",
    "fastq-scan: https://github.com/rpetit3/fastq-scan\n",
    "bavian en chroma\n",
    "Interpretatie van plotten, motivaties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality control "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12-03\n",
    "For quality control for our minION data, I choose minion_qc. As mentioned before this tools is quite effective and can handle big data. This tool can use sequence summary, so it is easier to use.\n",
    "I started making a workflow in github, with the folder workflow and in this folder with two folder named, rules and scripts. In the folder scripts I installed the R script of the minion_qc with wget.\n",
    "This is the wget command line I used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.2' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "wget https://raw.githubusercontent.com/roblanf/minion_qc/master/MinIONQC.R -O MinIONQC.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading it, make sure you have the following dependencies in your R. I went to R in linux and downloaded the following packages, otherwise minion_qc wouldn't work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(c(\"data.table\", \n",
    "                   \"futile.logger\",\n",
    "                   \"ggplot2\",\n",
    "                   \"optparse\",\n",
    "                   \"plyr\",\n",
    "                   \"readr\",\n",
    "                   \"reshape2\",\n",
    "                   \"scales\",\n",
    "                   \"viridis\",\n",
    "                   \"yaml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run minion_qc you should use the following in the command line.\n",
    "The input should be the path of input directory were it can find a file with the name sequence_summary.txt or give the path to the sequence_summary.txt.\n",
    "And for output it should have be the output directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rscript MinIONQC.R -i summary_sequence.txt -o output_minion_qc -s TRUE-p 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the command line there is -i, for the input path and -o for the output path. -s stands for if you want the plot sizes small, so you can use it in your research. If FALSE it will give the true size figures and TRUE will give smaller figure output. -p stands for number of processors you want to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rules folder I made an initial begin for minion_qc rule. \n",
    "And in the workflow folder I had the snakefile with rule all. So it will check for the output folder of what hte minion_qc makes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13-03\n",
    "With the github of minion_qc, I made the minionqc snakemake file, a snakefile with a rull all with the minionqc output file and a config yaml with paths to a directory. While running the snakefile, it gave an error with the command line 'snakemake --cores 2'. Firstly it gave that there was no output to run it. So it meant the rule all was not given correctly. So I thought maybe I should mkdir a folder in the output folder with the same name that the rule all expects with the input of the output folder. And if I made a folder, it says nothing the be done. This need to be checked again, as this means that the output folder is not made with the minion_qc rule. So something is wrong with the directory paths. Will look into it tomorrow and see if I missed something.\n",
    "\n",
    "14-03\n",
    "While working on why minion_qc is not working, I run the command line of the minion_qc with the paths I need in the linux portal if it actually works, or if I did something wrong. It actually worked but it couldnt plot two plots, as reshape2 melt didn't work. So I will look at the reshape2 after the snakefile works.\n",
    "I found the reason why the minion_qc rule did not work, it is because the path that was given in minion_qc, was not accesible in snakfile. So changed the place were the snakefile is to outside the workflow folder and I changed the output folder in minion_qc rule and in snakefile rule all. So it is more understandable. After running it again it worked.\n",
    "Afterwards I looked up why the reshape melt didn't work and searched up if someone had the same problem with minion_qc, but nobody mentioned it. Looked on stackoverflow if someone knew with reshape melt error, someone mentioned to specify in the script with reshape2::melt. After changing this in the Rscript of minIONQC.R and running it again, it gave all the plots it should give. As example you could see in the github of the minion_qc as it shows what output plots you should get.\n",
    "Looked at the plots, the quality of the data looked good. I will give the elaboration of the plots later. \n",
    "\n",
    "15-03\n",
    "The teacher said to write the logbook more extensiver and also writing in why and how you did it. So I rewrote the logbook in how the teacher said it should look like. I am not sure if this is the right way to write up the logbook. But will know later on, when the teacher comes again to check it. \n",
    "I don't remember exactly what I did before March 5th. As I did write the mails to our client, Jeroen, to make an appointment and mailed some question we had after our appointment. And also make question we could possibly ask and also literature research to know more about the subject.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![img channel_summary](../logbooks/channel_summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img flowcell_overview](../logbooks/flowcell_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img gb_per_channel_overview](../logbooks/gb_per_channel_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img q_histogram](../logbooks/q_histogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img q_by_hour](../logbooks/q_by_hour.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img length_by_hour](../logbooks/length_by_hour.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img length_histogram](../logbooks/length_histogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img yield_over_time](../logbooks/yield_over_time.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img reads_per_hour](../logbooks/reads_per_hour.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These figures shows that hte quality overall of the reads are good. And there is no need to trim. There are a few other figures that minion_qc produced. But those figures also show the same conclusion as the the quality is over good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HUMANN tool functional analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17-03\n",
    "From a classmate I heard if you want analyse pathways, that you should use Humann tool. So I searched the humann tool and come up with the bio.tool site \n",
    "https://bio.tools/humann \n",
    "*HUMAnN tool*\n",
    "HUMAnN is a pipeline to profile efficiently and accuratly to presence or absence of microbial paths from metagenomics sequencing data. So a tool that can be used after getting the Taxonomic classifiction with Kraken (ref: Tai logbook). \n",
    "https://huttenhower.sph.harvard.edu/humann2\n",
    "On this site, you could find more information about the HUMAnN. \n",
    "The article related to this is,\n",
    "https://www.nature.com/articles/s41592-018-0176-y.\n",
    "\n",
    "To install HUMAnN\n",
    "- pip install humann2\n",
    "- this pip install will install all the needed packages and will install also the new version.\n",
    "- Also need to download ChocoPhlAn database, as this database will organize microbial reference genomes according to their taxonomy\n",
    "- Also download UniRef database. \n",
    "\n",
    "Command line of Humann is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humann2 --input $sample - output $output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create temp files.\n",
    "$DIR/$SAMPLENAME_bowtie2_aligned.sam\n",
    "the full alignment output from bowtie2\n",
    "$DIR/$SAMPLENAME_bowtie2_aligned.tsv\n",
    "only the reduced aligned data from the bowtie2 output\n",
    "$DIR/$SAMPLENAME_bowtie2_index*\n",
    "bowtie2 index files created from the custom chochophlan database\n",
    "$DIR/$SAMPLENAME_bowtie2_unaligned.fa\n",
    "a fasta file of unaligned reads after the bowtie2 step\n",
    "$DIR/$SAMPLENAME_custom_chocophlan_database.ffn\n",
    "a custom chocophlan database of fasta sequences\n",
    "$DIR/$SAMPLENAME_metaphlan_bowtie2.txt\n",
    "the bowtie2 output from metaphlan\n",
    "$DIR/$SAMPLENAME_metaphlan_bugs_list.tsv\n",
    "the bugs list output from metaphlan\n",
    "$DIR/$SAMPLENAME_$TRANSLATEDALIGN_aligned.tsv\n",
    "the alignment results from the translated alignment step\n",
    "$DIR/$SAMPLENAME_$TRANSLATEDALIGN_unaligned.fa\n",
    "a fasta file of unaligned reads after the translated alignment step\n",
    "$DIR/$SAMPLENAME.log\n",
    "a log of the run\n",
    "$DIR=$OUTPUT_DIR/$SAMPLENAME_humann2_temp/\n",
    "$SAMPLENAME is the basename of the fastq/fasta input file\n",
    "$TRANSLATEDALIGN is the translated alignment software selected (rapsearch2 or usearch)\n",
    "\n",
    "I have installed the humann2 already to use it directly if the kraken2/bracken is finished\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20-03\n",
    "As the client gave that he wants to know more about three pathways. That might be available in the data. \n",
    "\n",
    "Fatty acid metabolism, central carbon metabolism and nitrogen fixation/ \n",
    "Fatty acid metabolism:\n",
    "Fatty acids are breaken down for energy (catabolism) and synthesized for storage or structural use (anabolism).\n",
    "\n",
    "Catabolism involves β-oxidation, where fatty acids are converted into acetyl-CoA, producing ATP, NADH, and FADH₂.\n",
    "\n",
    "Anabolism involves building fatty acids from acetyl-CoA using enzymes like fatty acid synthase, requiring ATP and NADPH.\n",
    "\n",
    "https://www.creative-proteomics.com/resource/what-is-fatty-acid-metabolism.htm\n",
    "\n",
    "Central carbon metabolism\n",
    "Central Carbon Metabolism (CCM) is a fundamental network of biochemical reactions that integrates key metabolic pathways—glycolysis, the tricarboxylic acid (TCA) cycle, and the pentose phosphate pathway—to convert nutrients into energy and essential biomolecules necessary for cellular growth and maintenance.\n",
    "\n",
    "Glycolysis: Breaks down glucose into pyruvate, yielding ATP and NADH.\n",
    "\n",
    "TCA Cycle: Processes acetyl-CoA derived from pyruvate to produce NADH and FADH₂, which are crucial for ATP generation.\n",
    "\n",
    "Pentose Phosphate Pathway: Generates NADPH and ribose-5-phosphate, supporting biosynthetic processes and maintaining redox balance.\n",
    "\n",
    "https://www.metwarebio.com/what-is-central-carbon-metabolism/\n",
    "\n",
    "\n",
    "Nitrogen Fixation \n",
    "The converstion of atmospheric nitron into ammonia, a form that is used by plants. \n",
    "This is carried out by specialized microooransims using the enxyme nitrogenase.\n",
    "Symbiosis process:\n",
    "Bacteria infect plant roots and form nodules.\n",
    "Inside nodules, N₂ is converted to NH₃.\n",
    "Plants use NH₃ for building proteins and nucleic acids.\n",
    "\n",
    "In return, bacteria receive carbon sources from the plant.\n",
    "https://www.ebsco.com/research-starters/agriculture-and-agribusiness/nitrogen-fixation\n",
    "\n",
    "There are three databases that Humann has, UniRef, MetaCyc and MinPath.\n",
    "UniRef database provides a gene family definitions. This database contains Uniproteinkb which protein.\n",
    "MetaCyc provides pathway definitions by gene family and gives elucidated metabolic pathways from all domains of life.\n",
    "MinPath is run to identify the set of minimum pathways. \n",
    "https://github.com/biobakery/humann?tab=readme-ov-file\n",
    "\n",
    "Humann uses MetaCyc as default. And we want to see the metabolic pathways from all domains of life. So we dont have to change the pathway database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I am using the humann tool and trying it firstly in command line as I wanted to test if it works. I take the fastq of barcode 01 and take a few lines to test the humann tool. To make the test file reference to Tai logbook. To use the humann, you needed uniref and chocophlan databases. I waited for Floris as he wanted to use the same tool Humann also for functional analysis, and as he was the person who told me about it. And if we can download the databse. He already downloaded it and so I used his database directory data/chocophlan and data/uniref.\n",
    "When I run it, it gave a problem as it had problem. After a while I saw it was that the directory path was incorrect. It was for chocophlan the wastewater/data/chocophlan/chocophlan and for uniref wastewater/data/uniref90_diamond/uniref. So after that I run it again and it works. However it takes a long time as it makes it own database taxonomic profile I did it for just one barcode test data as input. After searching it was apparent that you could give the kraken output the kreports as a taxonomic profile. But it have to me transformed into Metaphlan format.\n",
    "I tried to format it myself it didn't work. \n",
    "I made the humann snakemake file and it works with the snakemake rule all it gave some errors as i wanted to the config variable names. \n",
    "So i kept the full path of the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21-03\n",
    "Found a script of krakentools in github\n",
    "https://github.com/jenniferlu717/KrakenTools/blob/master/kreport2mpa.py. \n",
    "This person shows that you can make from a kreport to mpa. \n",
    "So I did it in command line first if it works. And it worked. \n",
    "As there is only one kraken barcode output for now I used that one to make a taxonomic profile to use Humann and so it wont take a long time.\n",
    "But it gave a problem as the format was not good, so i searched in the biobakery forum if someone had the same problem. \n",
    "Found someone that had the same problem. \n",
    "https://forum.biobakery.org/t/thoughts-on-custom-humann3-reference-databases/3383/6\n",
    "\n",
    "I tried it for the kraken barcode if it works. \n",
    "The person did it like this\n",
    "\n",
    "grep \"|s\" temp.MPA.TXT | sort -k 2 -r -n - | head -n $((top_bugs / 5)) - `#selects top 20 percent bugs`\\\n",
    "\t| awk '{printf(\"%s\\t\\n\", $0)}' - | awk 'BEGIN{printf(\"#mpa_v30_CHOCOPhlAn_201901\\n\")}1' - \\\n",
    "        > $__EXP_DIR/${__EXP_NAME}-bugs_list.MPA.TXT\n",
    "\n",
    "But I dont want to get the top 20 percent but every thing.\n",
    "I changed it till I had the good file I wanted so I removed the port of grep and head to select the top 20 and kept the sort and awk.\n",
    "I will look into it later . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22-03\n",
    "After I did that it worked.\n",
    "\n",
    "sort -k 2 -r -n {input.mpa_file} | \\\n",
    "        awk '{{printf(\"%s\\\\t\\\\n\", $0)}}' | \\\n",
    "        awk 'BEGIN{{printf(\"#mpa_v30_CHOCOPhlAn_201901\\\\n\")}}1' > {output.sorted_mpa_file} \\\n",
    "It took 45 minutes to run the humann for the test data and with the taxonmische profile.\n",
    "\n",
    "As it is a script and wanted to have it in the snakemake file, so it is a better pipeline and if someone wanted to use the same way as me it would be easier from them.\n",
    "The snakemake to transform kreports to mpa did not work well so it was a bit of a struggle. I will look into to it another time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24-03\n",
    "The problem with the snake make rule of kr2mpa was that the wildcards was not given correctly. I looked into the logs that wildcards and the file path of the output was not the correct way I changed it and had some other porblem with making this snakemake rule, so will look into it later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25-03\n",
    "I looked into the rule of krmpa the wildcards and paths were not given correctly so I had to change the wildcards.\n",
    "I added the sort rule in the snakemake rule of the kr2mpa and it sorts correctly. But it needs to be combined to be able to use it as a taxonomic profile. The same kraken tools git has a combine script, so I will try to tomorrow for the combine script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26-03\n",
    "I used the combine script for the kraken of mpa in command line and it works. So now it needs to be implemented in a snakemake rule. \n",
    "For the snakemake rule for this, was easy as it is almost the same as the kr2mpa. \n",
    "And I wanted to use the fastq files I firsty tried in command line and made a snakemake file. So if someone wanted to use it it is easier. \n",
    "Had some problem with the snakemake file of combine wgs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27-03\n",
    "The snakemake file rule of combine wgs worked. And I run the humann on the wgs data and changed the test data to combined data with the taxonomisch profile with the barcode01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
